---
title: ""
date: "`r Sys.Date()`"
output:
  rmdformats::readthedown:
    highlight: kate
---

```{r setup, include=FALSE}
## Global options
knitr::opts_chunk$set(cache = TRUE)
```

# Preparation

load packages

```{r}
library(brms)
library(tidyverse)
library(sjPlot)
library(performance)
library(rstan)
library(bayesplot)
library(loo)
library(bayestestR)
library(psych)
library(reshape2)
library(GGally)
library(ggridges)
library(DT)
library(knitr)
library(kableExtra)
```

```{r}
sessionInfo()
```

Set the theme

```{r}
theme_set(theme_bw())
set.seed(123)
rstan_options(auto_write = T)
options(mc.cores=parallel::detectCores())
```

load data

```{r}
dat <- read_csv("YesNo.csv")
```

convert to long format

```{r}
data_long <- dat %>% 
  pivot_longer(
    cols = -Participant,
    names_to = "Item",
    values_to = "Response") %>%
  mutate(
    Participant = factor(Participant))
```

make correct answers dataset

```{r}
item_list <- c("accumulation",	"accurate",	"adaptation",	"adjacent",	"allocation",	"alter",	"amendment",	"arbitrary",	"assembly",	"assurance",	"aware",	"behalf",	"capable",	"clarity",	"coincide",	"colleagues",	"comprise",	"concurrent",	"consultation",	"contradiction",	"conversely",	"crucial",	"denote",	"depression",	"deviation",	"discretion",	"discrimination",	"diversity",	"domain",	"duration",	"edition",	"eliminate",	"enable",	"energy",	"enforcement",	"enormous",	"equipment",	"equivalent",	"erosion",	"estate",	"ethical",	"exceed",	"explicit",	"exploitation",	"exposure",	"external",	"flexibility",	"forthcoming",	"foundation",	"gender",	"guidelines",	"hierarchical",	"incentive",	"incidence",	"inclination",	"incompatible",	"inherent",	"inhibition",	"initiatives",	"innovation",	"insights",	"inspection",	"instructions",	"integrity",	"intelligence",	"intensity",	"intervention",	"intrinsic",	"invoke",	"likewise",	"logic",	"manipulation",	"mediation",	"minimal",	"mode",	"nonetheless",	"nuclear",	"odd",	"ongoing",	"orientation",	"persistent",	"practitioners",	"protocol",	"publication",	"pursue",	"quotation",	"reluctant",	"restore",	"scenario",	"simulation",	"sphere",	"stability",	"straightforward",	"submitted",	"substitution",	"supplementary",	"symbolic",	"thereby",	"thesis",	"topic",	"transmission",	"undergo",	"unique",	"vehicle",	"visible",	"vision",	"visual",	"welfare",	"absolvention",	"ackery",	"almanical",	"amagran",	"amphlett",	"annobile",	"ashment",	"asslam",	"atribus",	"attard",	"bastionate",	"benevolate",	"berrow",	"cambule",	"captivise",	"carpin",	"causticate",	"charactal",	"coath",	"coppard",	"crucialate",	"cymballic",	"decorite",	"defunctionary",	"deliction",	"descript",	"doole",	"dring",	"dyment",	"ebullible",	"eluctant",	"expostulant",	"galpin",	"hapgood",	"hawther",	"hegedoxy",	"hermantic",	"hignall",	"hislop",	"hoult",	"interisation",	"jemmett",	"joice",	"keable",	"kearle",	"loveridge",	"majury",	"mastaphitis",	"neutration",	"nichee",	"oestrogeny",	"paralogue",	"perceptacle",	"peritonic",	"prowt",	"rainish",	"remonic",	"samphirate",	"savery",	"savourite",	"stattock",	"shuddery",	"tamnifest",	"tearle",	"tindle",	"transcendiary",	"vardy",	"viggers",	"warman",	"waygood",	"whitrow",	"wintle")
correct_answer_list <- c("Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"Yes",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No",	"No")
```

```{r}
correct_answers <- data.frame(
  Item = item_list,
  Correct_Answer = correct_answer_list)
```

combine data

```{r}
combined_data <- merge(data_long, correct_answers, by = "Item")
```

# Scoring Methods

- The Hit column is 1 when Response is "Yes" and Correct_Answer is "Yes"

- The Miss column is 1 when Response is "No" and Correct_Answer is "Yes"

- The False_Alarm column is 1 when Response is "Yes" and Correct_Answer is "No"

- The Correct_Rejection column is 1 when Response is "No" and Correct_Answer is "No"

- Scoring_1: 1 point if Hit is 1, 0 points for Correct_Rejection, Miss, and False_Alarm

- Scoring_2: 1 point if Hit is 1, 1 point if Correct_Rejection is 1, 0 points for Miss and False_Alarm

- Scoring_3: 2 points if Hit is 1, 1 point if Correct_Rejection is 1, 0 points for Miss and False_Alarm

- Scoring_4: 3 points if Hit is 1, 2 points if Correct_Rejection is 1, 1 point if Miss is 1, 0 points if False_Alarm is 1

```{r}
scoring_data <- combined_data %>%
  mutate(
    Hit = ifelse(Response == "Yes" & Correct_Answer == "Yes", 1, 0),
    Miss = ifelse(Response == "No" & Correct_Answer == "Yes", 1, 0),
    False_Alarm = ifelse(Response == "Yes" & Correct_Answer == "No", 1, 0),
    Correct_Rejection = ifelse(Response == "No" & Correct_Answer == "No", 1, 0),
    Scoring_1 = ifelse(Hit == 1, 1, 0),
    Scoring_2 = ifelse(Hit == 1 | Correct_Rejection == 1, 1, 0),
    Scoring_3 = ifelse(Hit == 1, 2, ifelse(Correct_Rejection == 1, 1, 0)),
    Scoring_4 = ifelse(Hit == 1, 2, ifelse(Correct_Rejection == 1, 1, ifelse(False_Alarm == 1, 0, 1))),
    Scoring_5 = ifelse(Hit == 1, 3, ifelse(Correct_Rejection == 1, 2, ifelse(False_Alarm == 1, 0, 1))))
```

convert to long format

```{r}
long_format_data <- scoring_data %>%
  pivot_longer(
    cols = c(Scoring_1, Scoring_2, Scoring_3, Scoring_4, Scoring_5),
    names_to = "Scoring_Method",
    values_to = "Score")

long_format_data <- mutate(
  long_format_data,
  Participant = factor(Participant),
  Scoring_Method = factor(Scoring_Method))
```

# Descriptive Statistics

```{r}
participant_scores <- long_format_data %>%
  group_by(Participant, Scoring_Method) %>%
  summarise(Total_Score = sum(Score, na.rm = T)) %>%
  pivot_wider(names_from = Scoring_Method, values_from = Total_Score)
```

```{r}
participant_scores %>%
  dplyr::select(Scoring_1, Scoring_2, Scoring_3, Scoring_4, Scoring_5) %>% 
  describe() %>% 
  kable(format = "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)
```

```{r}
long_data <- participant_scores %>%
  pivot_longer(cols = starts_with("Scoring"), names_to = "Scoring", values_to = "Score")

ggplot(long_data, aes(x = Score, y = ..density..)) +
  geom_histogram(binwidth = 10, alpha = 0.6) +
  geom_density(linewidth = 1) +
  facet_wrap(~ Scoring, scales = "free") +
  labs(title = "Histograms with Density Plots of Scoring Methods", x = "Score", y = "Density")
```

make data separate

```{r}
scoring_1_data <- long_format_data %>%
  filter(Scoring_Method == "Scoring_1") %>% 
  filter(Correct_Answer == "Yes")

scoring_2_data <- long_format_data %>%
  filter(Scoring_Method == "Scoring_2") %>%
  mutate(Score = as.ordered(Score))

scoring_3_data <- long_format_data %>%
  filter(Scoring_Method == "Scoring_3") %>%
  mutate(Score = as.ordered(Score))

scoring_4_data <- long_format_data %>%
  filter(Scoring_Method == "Scoring_4") %>%
  mutate(Score = as.ordered(Score))

scoring_5_data <- long_format_data %>%
  filter(Scoring_Method == "Scoring_5") %>%
  mutate(Score = as.ordered(Score))
```

```{r}
datatable(
  scoring_1_data,
  filter='top', extensions = 'Scroller', class="compact",
  options = list(scrollY = 400, searching = T, paging = F,
                 columnDefs = list(list(className = 'dt-left', targets = "_all"))))
```

```{r}
datatable(
  scoring_2_data,
  filter='top', extensions = 'Scroller', class="compact",
  options = list(scrollY = 400, searching = T, paging = F,
                 columnDefs = list(list(className = 'dt-left', targets = "_all"))))
```

```{r}
datatable(
  scoring_3_data,
  filter='top', extensions = 'Scroller', class="compact",
  options = list(scrollY = 400, searching = T, paging = F,
                 columnDefs = list(list(className = 'dt-left', targets = "_all"))))
```

```{r}
datatable(
  scoring_4_data,
  filter='top', extensions = 'Scroller', class="compact",
  options = list(scrollY = 400, searching = T, paging = F,
                 columnDefs = list(list(className = 'dt-left', targets = "_all"))))
```

```{r}
datatable(
  scoring_5_data,
  filter='top', extensions = 'Scroller', class="compact",
  options = list(scrollY = 400, searching = T, paging = F,
                 columnDefs = list(list(className = 'dt-left', targets = "_all"))))
```

## Meara's (1996) Δm and Huibregtse et al.'s (2002) `I_{SDT}`

Below are the equations for `Δm` and `I_{SDT}`:

$$
\Delta m = \frac{h - f}{1 - f} - \frac{f}{h}
$$

$$
I_{SDT} = 1 - \frac{4h (1 - f) - 2 (h - f) (1 + h - f)}{4h (1 - f) - (h - f) (1 + h - f)}
$$

prepare data

```{r}
previous_scoring_data <- combined_data %>%
  mutate(
    Hit = ifelse(Response == "Yes" & Correct_Answer == "Yes", 1, 0),
    Miss = ifelse(Response == "No" & Correct_Answer == "Yes", 1, 0),
    False_Alarm = ifelse(Response == "Yes" & Correct_Answer == "No", 1, 0),
    Correct_Rejection = ifelse(Response == "No" & Correct_Answer == "No", 1, 0))

participant_rates <- previous_scoring_data %>%
  group_by(Participant) %>%
  summarise(
    h = sum(Hit) / sum(Correct_Answer == "Yes"),
    f = sum(False_Alarm) / sum(Correct_Answer == "No"))

participant_rates <- participant_rates %>%
  mutate(
    delta_m = (h - f) / (1 - f) - (f / h),
    I_SDT = 1 - (4 * h * (1 - f) - 2 * (h - f) * (1 + h - f)) / (4 * h * (1 - f) - (h - f) * (1 + h - f)))
```

descriptive statistics

```{r}
describe(participant_rates) %>%
  kable(format = "html") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)
```

## Correlation

```{r}
combined_data <- participant_rates %>%
  inner_join(participant_scores, by = "Participant")
```

```{r}
cor_matrix <- combined_data %>%
  select(h, f, delta_m, I_SDT, Scoring_1, Scoring_2, Scoring_3, Scoring_4, Scoring_5) %>%
  cor()
```

```{r}
sjPlot::tab_corr(
  cor_matrix, triangle = "lower", 
  title = "Correlation Matrix",
  var.labels = c("Hit Rate", "False Alarm Rate", "Delta_m", "I_SDT", "Scoring_1", "Scoring_2", "Scoring_3", "Scoring_4", "Scoring_5"))
```

```{r}
ggpairs(
  combined_data, columns = 2:10, 
  title = "Correlation Matrix",
  upper = list(continuous = "cor"),
  lower = list(continuous = "smooth"))
```

# Bayesian Estimation

## set prior distribution and model formula

The prior distribution is set as a normal distribution with a mean of 0 and a standard deviation of 3, based on Levy and Miskevy (2017) and Bürkner (2017). Additionally, separate prior distributions are set for Participant and Item, and partial pooling is employed.

```{r}
prior <- set_prior("normal(0, 3)", class = "sd", group = "Participant") +
  set_prior("normal(0, 3)", class = "sd", group = "Item")
```

## model specification

The model is specified as a multilevel logistic regression model with a random intercept for Participant and Item. The outcome variable is Score, which are dichotomous test data in the YesNo vocabulary test.

1PLM is represented as follows:

$$
f(θ_{kpn} + ξ_{kin})=logistic(θ_{kpn} + ξ_{kin})=\frac{exp(θ_{kpn} + ξ_{kin})}{1+exp(θ_{kpn} + ξ_{kin})} = ψ_{kn}
$$

```{r}
fit_scoring_1 <- brm(
  Score ~ 1 + (1 | Item) + (1 | Participant),
  data = scoring_1_data,
  family = bernoulli(link = "logit"),
  prior = prior,
  save_pars = save_pars(all = TRUE),
  seed = 1234)

fit_scoring_2 <- brm(
  Score ~ 1 + (1 | Item) + (1 | Participant),
  data = scoring_2_data,
  family = bernoulli(link = "logit"),
  prior = prior,
  save_pars = save_pars(all = TRUE),
  seed = 1234)
```

# Stan Code

```{r}
make_stancode(fit_scoring_1)
```

```{r}
make_stancode(fit_scoring_2)
```

# Model Results

```{r}
summary(fit_scoring_1)
```

```{r}
summary(fit_scoring_2)
```

# Fit Statistics

extract item and person parameters

```{r}
ranef_scoring_1 <- ranef(fit_scoring_1)
ranef_scoring_2 <- ranef(fit_scoring_2)
```

item parameter

```{r}
item_pars_scoring_1 <- ranef_scoring_1$Item
item_pars_scoring_2 <- ranef_scoring_2$Item
```

person parameter

```{r}
person_pars_scoring_1 <- ranef_scoring_1$Participant
person_pars_scoring_2 <- ranef_scoring_2$Participant

coef_scoring_1 <- coef(fit_scoring_1)$Participant
coef_scoring_2 <- coef(fit_scoring_2)$Participant
```

```{r}
# 項目の推定値を抽出
item_pars_scoring_1_estimate <- item_pars_scoring_1[, "Estimate", 1]

# 項目名を取得
item_names <- dimnames(item_pars_scoring_1)[[1]]

# データフレームに変換
item_data <- data.frame(Parameter = item_pars_scoring_1_estimate, Item = item_names, Type = "Item")

# 受験者の推定値を抽出
person_pars_scoring_1_estimate <- person_pars_scoring_1[, "Estimate", 1]

# 受験者名を取得
person_names <- dimnames(person_pars_scoring_1)[[1]]

# データフレームに変換
person_data <- data.frame(Parameter = person_pars_scoring_1_estimate, Participant = person_names, Type = "Person")
```

```{r}
# 列名を統一
colnames(item_data)[colnames(item_data) == "Item"] <- "Name"
colnames(person_data)[colnames(person_data) == "Participant"] <- "Name"

# データを結合
all_data <- rbind(item_data, person_data)

# Extract the first dimension of item_pars_scoring_1, which corresponds to "Estimate"
item_data <- data.frame(Parameter = item_pars_scoring_1[, "Estimate", 1], 
                        Name = rownames(item_pars_scoring_1), 
                        Type = "Item")

# Extract the first dimension of person_pars_scoring_1, which corresponds to "Estimate"
person_data <- data.frame(Parameter = person_pars_scoring_1[, "Estimate", 1], 
                          Name = rownames(person_pars_scoring_1), 
                          Type = "Person")

# Combine the person and item data
combined_data <- rbind(item_data, person_data)

# ヒストグラムを左右に分けて描画
ggplot(combined_data, aes(x = Parameter, fill = Type)) +
  geom_histogram(data = subset(combined_data, Type == "Item"), aes(y = ..count..),
                 binwidth = 0.5, alpha = 0.7) +
  geom_histogram(data = subset(combined_data, Type == "Person"), aes(y = -..count..),
                 binwidth = 0.5, alpha = 0.7) +
  labs(x = "θ(Logit)", y = "Frequency") +
  coord_flip() +
  theme_bw()
```


predict the probability of correct response

```{r}
predicted_probs_1 <- predict(fit_scoring_1, type = "response")
predicted_probs_2 <- predict(fit_scoring_2, type = "response")
```

calculate residuals

```{r}
numeric_score_1 <- as.numeric(as.character(fit_scoring_1$data$Score))
numeric_score_2 <- as.numeric(as.character(fit_scoring_2$data$Score))

residuals_1 <- numeric_score_1 - predicted_probs_1[, "Estimate"]
residuals_2 <- numeric_score_2 - predicted_probs_2[, "Estimate"]
```

item information function

```{r}
info_function_1 <- predicted_probs_1[, "Estimate"] * (1 - predicted_probs_1[, "Estimate"])
info_function_2 <- predicted_probs_2[, "Estimate"] * (1 - predicted_probs_2[, "Estimate"])
```

outfit and infit statistics

$$
Outfit=\frac{1}{N}\sum_{i=1}^{N} \frac{(U_i - P_{ij})^2}{P_{ij}(1 - P_{ij})}
$$

$$
Infit= \frac{\sum_{i=1}^{N} (U_{ij} - P_{ij})^2}{\sum_{i=1}^{N} P_{ij}(1 - P_{ij})}
$$

```{r}
calculate_fit_stats_1 <- function(residuals_1, info_function_1) {
  N <- length(residuals_1)
  
  outfit_ms <- mean(residuals_1^2 / info_function_1)
  outfit_sd <- sqrt((sum(1/info_function_1^2) / N^2) - 4/N)
  
  infit_ms <- sum(residuals_1^2) / sum(info_function_1)
  infit_sd <- sqrt((sum(info_function_1) - 4 * sum(info_function_1^2)) / sum(info_function_1)^2)
  
  outfit_t <- ((outfit_ms^(1/3) - 1) * (3/outfit_sd)) + (outfit_sd/3)
  infit_t <- ((infit_ms^(1/3) - 1) * (3/infit_sd)) + (infit_sd/3)
  
  return(c(outfit_ms = outfit_ms, outfit_t = outfit_t,
           infit_ms = infit_ms, infit_t = infit_t))
}

calculate_fit_stats_2 <- function(residuals_2, info_function_2) {
  N <- length(residuals_2)
  
  outfit_ms <- mean(residuals_2^2 / info_function_2)
  outfit_sd <- sqrt((sum(1/info_function_2^2) / N^2) - 4/N)
  
  infit_ms <- sum(residuals_2^2) / sum(info_function_2)
  infit_sd <- sqrt((sum(info_function_2) - 4 * sum(info_function_2^2)) / sum(info_function_2)^2)
  
  outfit_t <- ((outfit_ms^(1/3) - 1) * (3/outfit_sd)) + (outfit_sd/3)
  infit_t <- ((infit_ms^(1/3) - 1) * (3/infit_sd)) + (infit_sd/3)
  
  return(c(outfit_ms = outfit_ms, outfit_t = outfit_t,
           infit_ms = infit_ms, infit_t = infit_t))
}

item_fit_stats_1 <- fit_scoring_1$data %>%
  group_by(Item) %>%
  summarise(
    fit_stats_1 = list(calculate_fit_stats_1(residuals_1[cur_group_rows()], info_function_1[cur_group_rows()])),
    n = n()
  ) %>%
  tidyr::unnest_wider(fit_stats_1)

person_fit_stats_1 <- fit_scoring_1$data %>%
  group_by(Participant) %>%
  summarise(
    fit_stats_1 = list(calculate_fit_stats_1(residuals_1[cur_group_rows()], info_function_1[cur_group_rows()])),
    n = n()
  ) %>%
  tidyr::unnest_wider(fit_stats_1)

item_fit_stats_2 <- fit_scoring_2$data %>%
  group_by(Item) %>%
  summarise(
    fit_stats_2 = list(calculate_fit_stats_2(residuals_2[cur_group_rows()], info_function_2[cur_group_rows()])),
    n = n()
  ) %>%
  tidyr::unnest_wider(fit_stats_2)

person_fit_stats_2 <- fit_scoring_2$data %>%
  group_by(Participant) %>%
  summarise(
    fit_stats_2 = list(calculate_fit_stats_2(residuals_2[cur_group_rows()], info_function_2[cur_group_rows()])),
    n = n()
  ) %>%
  tidyr::unnest_wider(fit_stats_2)
```

## Visualization

### Distribution of Infit Mean Square

Scoring_1

```{r}
ggplot(person_fit_stats_1, aes(x = infit_ms)) +
  geom_histogram(binwidth = 0.05, alpha = 0.7) +
  geom_vline(xintercept = c(0.5, 1.5), linetype = "dashed", color = "red") +
  labs(title = "Distribution of Infit Mean Square (Participants): Scoring_1",
       x = "Infit Mean Square", y = "Count")

ggplot(item_fit_stats_1, aes(x = infit_ms)) +
  geom_histogram(binwidth = 0.05, alpha = 0.7) +
  geom_vline(xintercept = c(0.5, 1.5), linetype = "dashed", color = "red") +
  labs(title = "Distribution of Infit Mean Square (Items): Scoring_1",
       x = "Infit Mean Square", y = "Count")

ggplot(item_fit_stats_1, aes(x = infit_ms, y = outfit_ms)) +
  geom_point(alpha = 0.7) +
  geom_text(aes(label = Item), vjust = -0.5, hjust = 0.5, size = 3) +
  geom_hline(yintercept = c(0.5, 1.5), linetype = "dashed", color = "red") +
  geom_vline(xintercept = c(0.5, 1.5), linetype = "dashed", color = "red") +
  labs(title = "Infit vs Outfit Mean Square (Items): Scoring_1",
       x = "Infit Mean Square", y = "Outfit Mean Square")
```

Scoring_2

```{r}
ggplot(person_fit_stats_2, aes(x = infit_ms)) +
  geom_histogram(binwidth = 0.05, alpha = 0.7) +
  geom_vline(xintercept = c(0.5, 1.5), linetype = "dashed", color = "red") +
  labs(title = "Distribution of Infit Mean Square (Participants): Scoring_2",
       x = "Infit Mean Square", y = "Count")

ggplot(item_fit_stats_2, aes(x = infit_ms)) +
  geom_histogram(binwidth = 0.05, alpha = 0.7) +
  geom_vline(xintercept = c(0.5, 1.5), linetype = "dashed", color = "red") +
  labs(title = "Distribution of Infit Mean Square (Items): Scoring_2",
       x = "Infit Mean Square", y = "Count")

ggplot(item_fit_stats_2, aes(x = infit_ms, y = outfit_ms)) +
  geom_point(alpha = 0.7) +
  geom_text(aes(label = Item), vjust = -0.5, hjust = 0.5, size = 3) +
  geom_hline(yintercept = c(0.5, 1.5), linetype = "dashed", color = "red") +
  geom_vline(xintercept = c(0.5, 1.5), linetype = "dashed", color = "red") +
  labs(title = "Infit vs Outfit Mean Square (Items): Scoring_2",
       x = "Infit Mean Square", y = "Outfit Mean Square")
```

# Refit the model with removing outliers

list of items to remove

```{r}
items_to_remove <- c("topic", "quotation", "invention", "discretion", "hignall", "descript")
```

New Scoring_1

```{r}
scoring_1_data_filtered <- scoring_1_data %>%
  filter(!Item %in% items_to_remove)

(n_removed <- sum(scoring_1_data$Item %in% items_to_remove))
(n_remaining <- nrow(scoring_1_data_filtered))

(n_unique_items_before <- length(unique(scoring_1_data$Item)))
(n_unique_items_after <- length(unique(scoring_1_data_filtered$Item)))
```

```{r}
scoring_2_data_filtered <- scoring_2_data %>%
  filter(!Item %in% items_to_remove)

(n_removed <- sum(scoring_2_data$Item %in% items_to_remove))
(n_remaining <- nrow(scoring_2_data_filtered))

(n_unique_items_before <- length(unique(scoring_2_data$Item)))
(n_unique_items_after <- length(unique(scoring_2_data_filtered$Item)))
```


```{r}
fit_scoring_1 <- brm(
  Score ~ 1 + (1 | Item) + (1 | Participant),
  data = scoring_1_data_filtered,
  family = bernoulli(link = "logit"),
  prior = prior,
  save_pars = save_pars(all = TRUE),
  seed = 1234)

fit_scoring_2 <- brm(
  Score ~ 1 + (1 | Item) + (1 | Participant),
  data = scoring_2_data_filtered,
  family = bernoulli(link = "logit"),
  prior = prior,
  save_pars = save_pars(all = TRUE),
  seed = 1234)
```

# Stan Code

```{r}
make_stancode(fit_scoring_1)
```

```{r}
make_stancode(fit_scoring_2)
```

# Model Results

```{r}
summary(fit_scoring_1)
```

```{r}
summary(fit_scoring_2)
```

# Model Comparison

The Leave-One-Out Cross-Validation (LOO-CV) method involves training a model on a dataset excluding one data point, then making a prediction on the excluded data point. This process is repeated for each data point in the dataset.

Consider a dataset
$$
D = \{(x₁, y₁), \dots, (xₙ, yₙ)\} 
$$).

The LOO-CV estimate is expressed as follows:

$$
\text{LOO-CV} = -2 \sum_{i=1}^n \log p(y_i | x_i, D_{(-i)})
$$

```{r}
loo_cv_scoring_1 <- brms::loo(fit_scoring_1)
loo_cv_scoring_2 <- brms::loo(fit_scoring_2)
```

```{r}
print(loo_cv_scoring_1, simplify = FALSE)
print(loo_cv_scoring_2, simplify = FALSE)
```

The Widely Applicable Information Criterion (WAIC) is one of the information criteria used to evaluate the predictive performance of Bayesian models. It is asymptotically equivalent to Leave-One-Out Cross-Validation (LOO-CV) and is widely used due to its relatively easy computation.

WAIC is defined by the following formula:

$$
\text{WAIC} = -2(\text{lppd} - p_{\text{WAIC}})
$$

- lppd is the log pointwise predictive density.

- p_WAIC is the effective number of parameters (a penalty term for model complexity).

```{r}
waic_1 <- brms::waic(fit_scoring_1)
waic_2 <- brms::waic(fit_scoring_2)
```

```{r}
print(waic_1$estimates)
print(waic_2$estimates)
```

# Visualization

## trace plot

```{r}
plot(fit_scoring_1, combo = c("trace", "dens_overlay"), ask = F)
plot(fit_scoring_2, combo = c("trace", "dens_overlay"), ask = F)
```

## posterior predictive checks

```{r}
pp_check(
  object = fit_scoring_1,
  ndraws = 1000,
  type = "stat",
  stat = "mean") +
  theme_test() +
  ylab("Frequency")

pp_check(
  object = fit_scoring_1,
  ndraws = 100) +
  theme_test() +
  ylab("Density")

pp_check(
  object = fit_scoring_2,
  ndraws = 1000,
  type = "stat",
  stat = "mean") +
  theme_test() +
  ylab("Frequency")

pp_check(
  object = fit_scoring_2,
  ndraws = 100) +
  theme_test() +
  ylab("Density")
```


## Visualization

### Scoring for ONLY Hit

```{r}
ranef_scoring_1$Participant[, , "Intercept"] %>%
  as_tibble() %>%
  rownames_to_column() %>%
  arrange(Estimate) %>%
  mutate(rowname = seq_len(n())) %>%
  ggplot(aes(rowname, Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_pointrange(alpha = 0.7) +
  coord_flip() +
  labs(x = "Person Number (Sorted)")
```

```{r}
ranef_scoring_1$Item[, , "Intercept"] %>%
  as_tibble() %>%
  rownames_to_column() %>%
  arrange(Estimate) %>%
  mutate(rowname = seq_len(n())) %>%
  ggplot(aes(rowname, Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_pointrange(alpha = 0.7) +
  coord_flip() +
  labs(x = "Item Number (Sorted)")
```

### Scoring Method in Previous Research

```{r}
ranef_scoring_2$Participant[, , "Intercept"] %>%
  as_tibble() %>%
  rownames_to_column() %>%
  arrange(Estimate) %>%
  mutate(rowname = seq_len(n())) %>%
  ggplot(aes(rowname, Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_pointrange(alpha = 0.7) +
  coord_flip() +
  labs(x = "Person Number (Sorted)")
```

```{r}
ranef_scoring_2$Item[, , "Intercept"] %>%
  as_tibble() %>%
  rownames_to_column() %>%
  arrange(Estimate) %>%
  mutate(rowname = seq_len(n())) %>%
  ggplot(aes(rowname, Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_pointrange(alpha = 0.7) +
  coord_flip() +
  labs(x = "Item Number (Sorted)")
```

### Test Information Function

```{r}
calculate_p_correct <- function(theta, b, a = 1) {
  1 / (1 + exp(-a * (theta + b)))
}

calculate_item_info <- function(theta, b, a = 1) {
  p <- calculate_p_correct(theta, b, a)
  a^2 * p * (1 + p)
}

calculate_test_info <- function(theta, item_params) {
  sapply(theta, function(t) sum(mapply(calculate_item_info, t, item_params)))
}

create_plot_data <- function(theta, selected_items, ranef_data_1, ranef_data_2) {
  item_diff_1 <- ranef_data_1$Item[selected_items, , 1, drop = TRUE]
  item_diff_2 <- ranef_data_2$Item[selected_items, , 1, drop = TRUE]
  
  plot_data <- expand.grid(theta = theta, item = selected_items, scoring = c("Scoring_1", "Scoring_2"))
  
  plot_data <- plot_data %>%
    mutate(
      difficulty = case_when(
        scoring == "Scoring_1" ~ item_diff_1[match(as.character(item), selected_items), "Estimate"],
        scoring == "Scoring_2" ~ item_diff_2[match(as.character(item), selected_items), "Estimate"]),
      lower_ci = case_when(
        scoring == "Scoring_1" ~ item_diff_1[match(as.character(item), selected_items), "Q2.5"],
        scoring == "Scoring_2" ~ item_diff_2[match(as.character(item), selected_items), "Q2.5"]),
      upper_ci = case_when(
        scoring == "Scoring_1" ~ item_diff_1[match(as.character(item), selected_items), "Q97.5"],
        scoring == "Scoring_2" ~ item_diff_2[match(as.character(item), selected_items), "Q97.5"]),
      p = mapply(calculate_p_correct, theta, difficulty),
      p_lower = mapply(calculate_p_correct, theta, lower_ci),
      p_upper = mapply(calculate_p_correct, theta, upper_ci),
      info = mapply(calculate_item_info, theta, difficulty),
      info_lower = mapply(calculate_item_info, theta, lower_ci),
      info_upper = mapply(calculate_item_info, theta, upper_ci))
  
  return(plot_data)
}

create_tif_data <- function(theta, ranef_data_1, ranef_data_2) {
  calculate_tif_with_ci <- function(theta, ranef_data) {
    item_params <- ranef_data$Item[, c("Estimate", "Q2.5", "Q97.5"), 1]
    test_info <- apply(item_params, 2, function(params) calculate_test_info(theta, params))
    data.frame(theta = theta, 
               test_info = test_info[, "Estimate"],
               lower_ci = test_info[, "Q2.5"],
               upper_ci = test_info[, "Q97.5"])
  }
  
  bind_rows(
    calculate_tif_with_ci(theta, ranef_data_1) %>% mutate(scoring = "Scoring_1"),
    calculate_tif_with_ci(theta, ranef_data_2) %>% mutate(scoring = "Scoring_2"))
}

create_diff_data <- function(ranef_data_1, ranef_data_2) {
  bind_rows(
    data.frame(ranef_data_1$Item[, , 1], item = rownames(ranef_data_1$Item), scoring = "Scoring_1"),
    data.frame(ranef_data_2$Item[, , 1], item = rownames(ranef_data_2$Item), scoring = "Scoring_2")) %>%
    select(item, Estimate, Q2.5, Q97.5, scoring)
}

create_comparison_plots <- function(ranef_scoring_1, ranef_scoring_2, num_items = 5) {
  common_items <- intersect(rownames(ranef_scoring_1$Item), rownames(ranef_scoring_2$Item))
  selected_items <- sample(common_items, min(num_items, length(common_items)))
  
  theta <- seq(-6, 6, length.out = 100)
  
  plot_data <- create_plot_data(theta, selected_items, ranef_scoring_1, ranef_scoring_2)
  tif_data <- create_tif_data(theta, ranef_scoring_1, ranef_scoring_2)
  diff_data <- create_diff_data(ranef_scoring_1, ranef_scoring_2)
  
  icc_plot <- create_icc_plot(plot_data)
  iic_plot <- create_iic_plot(plot_data)
  tif_plot <- create_tif_plot(tif_data)
  diff_plot <- create_diff_plot(diff_data)
  
  list(icc = icc_plot, iic = iic_plot, tif = tif_plot, difficulty = diff_plot)
}

create_icc_plot <- function(plot_data) {
  ggplot(plot_data, aes(x = theta, y = p, color = item, linetype = scoring)) +
    geom_line() +
    geom_ribbon(aes(ymin = p_lower, ymax = p_upper, fill = item), alpha = 0.1, color = NA) +
    labs(title = "Item Characteristic Curves",
         x = "Latent Trait (θ)", y = "Probability of Correct Response") 
}

create_iic_plot <- function(plot_data) {
  ggplot(plot_data, aes(x = theta, y = info, color = item, linetype = scoring)) +
    geom_line() +
    geom_ribbon(aes(ymin = info_lower, ymax = info_upper, fill = item), alpha = 0.1, color = NA) +
    labs(title = "Item Information Curves",
         x = "Latent Trait (θ)", y = "Item Information")
}

create_tif_data <- function(theta, ranef_data_1, ranef_data_2) {
  calculate_tif_with_ci <- function(theta, ranef_data) {
    item_params <- ranef_data$Item[, c("Estimate", "Q2.5", "Q97.5"), 1, drop = FALSE]
    test_info <- apply(item_params, 2, function(params) calculate_test_info(theta, params))
    data.frame(theta = theta, 
               test_info = test_info[, "Estimate"],
               lower_ci = test_info[, "Q2.5"],
               upper_ci = test_info[, "Q97.5"])
  }
  
  bind_rows(
    calculate_tif_with_ci(theta, ranef_data_1) %>% mutate(scoring = "Scoring_1"),
    calculate_tif_with_ci(theta, ranef_data_2) %>% mutate(scoring = "Scoring_2"))
}

create_tif_plot <- function(tif_data) {
  ggplot(tif_data, aes(x = theta, y = test_info, color = scoring)) +
    geom_line() +
    geom_ribbon(aes(ymin = lower_ci, ymax = upper_ci, fill = scoring), alpha = 0.1, color = NA) +
    labs(title = "Test Information Function",
         x = "Latent Trait (θ)", y = "Test Information")
}

create_diff_data <- function(ranef_data_1, ranef_data_2) {
  bind_rows(
    as.data.frame(ranef_data_1$Item[, , 1]) %>% 
      mutate(item = rownames(ranef_data_1$Item), scoring = "Scoring_1"),
    as.data.frame(ranef_data_2$Item[, , 1]) %>% 
      mutate(item = rownames(ranef_data_2$Item), scoring = "Scoring_2")) %>%
    select(item, Estimate, Q2.5, Q97.5, scoring)
}

create_diff_plot <- function(diff_data) {
  ggplot(diff_data, aes(x = Estimate, y = reorder(item, Estimate), color = scoring)) +
    geom_point(position = position_dodge(width = 0.5)) +
    geom_errorbarh(aes(xmin = Q2.5, xmax = Q97.5), height = 0.2, position = position_dodge(width = 0.5)) +
    labs(title = "Item Difficulties Comparison",
         x = "Difficulty (b-parameter)", y = "Item")
}

create_comparison_plots <- function(ranef_scoring_1, ranef_scoring_2, num_items = 5) {
  common_items <- intersect(rownames(ranef_scoring_1$Item), rownames(ranef_scoring_2$Item))
  selected_items <- sample(common_items, min(num_items, length(common_items)))
  
  theta <- seq(-6, 6, length.out = 100)
  
  plot_data <- create_plot_data(theta, selected_items, ranef_scoring_1, ranef_scoring_2)
  tif_data <- create_tif_data(theta, ranef_scoring_1, ranef_scoring_2)
  diff_data <- create_diff_data(ranef_scoring_1, ranef_scoring_2)
  
  icc_plot <- create_icc_plot(plot_data)
  iic_plot <- create_iic_plot(plot_data)
  tif_plot <- create_tif_plot(tif_data)
  diff_plot <- create_diff_plot(diff_data)
  
  list(icc = icc_plot, iic = iic_plot, tif = tif_plot, difficulty = diff_plot)
}
```

```{r}
create_comparison_plots(ranef_scoring_1, ranef_scoring_2)
```


### Item Information Function

Define ability range

```{r}
theta_range <- seq(-6, 6, length.out = 100)
```

Define item information function

```{r}
item_info <- function(b, theta) {
  p_theta <- 1 / (1 + exp(-(theta + b)))
  return(p_theta * (1 - p_theta))
}
```

Calculate item information for each item

```{r}
# Calculate item information for each item
calc_item_info <- function(b_participant, theta_range) {
  info_list <- lapply(seq_along(theta_range), function(t) {
    theta = theta_range[t]
    mean_info = sum(sapply(b_participant[, "Estimate"], item_info, theta = theta))
    lower_info = sum(sapply(b_participant[, "Q2.5"], item_info, theta = theta))
    upper_info = sum(sapply(b_participant[, "Q97.5"], item_info, theta = theta))
    return(c(mean = mean_info, lower = lower_info, upper = upper_info))
  })
  return(do.call(rbind, info_list))
}

# Extract participant ability parameters for each scoring method
b_participant_scoring_1 <- coef(fit_scoring_1)$Participant[, ,"Intercept"]
b_participant_scoring_2 <- coef(fit_scoring_2)$Participant[, ,"Intercept"]

# Calculate item information for each scoring method
info_results_scoring_1 <- calc_item_info(b_participant_scoring_1, theta_range)
info_results_scoring_2 <- calc_item_info(b_participant_scoring_2, theta_range)

# Convert the results to data frames for plotting
info_df_scoring_1 <- as.data.frame(info_results_scoring_1)
info_df_scoring_1$theta <- theta_range

info_df_scoring_2 <- as.data.frame(info_results_scoring_2)
info_df_scoring_2$theta <- theta_range

info_df_scoring_1 <- info_df_scoring_1 %>%
  mutate(lower = pmin(lower, mean),
         upper = pmax(upper, mean))

info_df_scoring_2 <- info_df_scoring_2 %>%
  mutate(lower = pmin(lower, mean),
         upper = pmax(upper, mean))

# maximum test information and theta
max_scoring_1 <- max(info_df_scoring_1$upper)
max_scoring_2 <- max(info_df_scoring_2$upper)

max_scoring_1_row <- info_df_scoring_1[info_df_scoring_1$upper == max_scoring_1, ]
max_scoring_2_row <- info_df_scoring_2[info_df_scoring_2$upper == max_scoring_2, ]

max_theta_scoring_1 <- max_scoring_1_row$theta
max_theta_scoring_2 <- max_scoring_2_row$theta

max_mean_scoring_1 <- max_scoring_1_row$mean
max_mean_scoring_2 <- max_scoring_2_row$mean
```

#### The aximum test information score and its theta

```{r}
cat("Theta for maximum test information in Scoring 1:", max_theta_scoring_1, "\nMean for maximum test information in Scoring 1:", max_mean_scoring_1)
cat("\nTheta for maximum test information in Scoring 2:", max_theta_scoring_2, "\nMean for maximum test information in Scoring 2:", max_mean_scoring_2)
```

```{r}
# Add a scoring method identifier to each data frame
info_df_scoring_1 <- info_df_scoring_1 %>% mutate(Scoring_Method = "Scoring_1")
info_df_scoring_2 <- info_df_scoring_2 %>% mutate(Scoring_Method = "Scoring_2")

combined_info_df <- bind_rows(info_df_scoring_1, info_df_scoring_2)

ggplot(combined_info_df, aes(x = theta, y = mean, color = Scoring_Method, fill = Scoring_Method)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
  labs(title = "Item Information Function by Scoring Method", x = "Theta", y = "Information")
```

# Sensitivity Analysis

prior distributions' SDs changes from 1 to 5

```{r}
prior_settings <- list(
  prior_normal_1 = set_prior("normal(0, 1)", class = "sd", group = "Participant") +
    set_prior("normal(0, 1)", class = "sd", group = "Item"),
  prior_normal_2 = set_prior("normal(0, 2)", class = "sd", group = "Participant") +
    set_prior("normal(0, 2)", class = "sd", group = "Item"),
  prior_normal_3 = set_prior("normal(0, 3)", class = "sd", group = "Participant") +
    set_prior("normal(0, 3)", class = "sd", group = "Item"),
  prior_normal_4 = set_prior("normal(0, 4)", class = "sd", group = "Participant") +
    set_prior("normal(0, 4)", class = "sd", group = "Item"),
  prior_normal_5 = set_prior("normal(0, 5)", class = "sd", group = "Participant") +
    set_prior("normal(0, 5)", class = "sd", group = "Item"))

```

scoring data list

```{r}
scoring_data_list <- list(
  scoring_1 = scoring_1_data,
  scoring_2 = scoring_2_data)

model_results <- list()
ranef_results <- list()

for (scoring_method in names(scoring_data_list)) {
  for (prior_setting in names(prior_settings)) {
    if (scoring_method %in% c("scoring_1", "scoring_2")) {
      family <- bernoulli(link = "logit")
    } else {
      family <- cumulative(link = "logit")
    }
    
    model_1pl <- brm(
      Score ~ 1 + (1 | Item) + (1 | Participant),
      data = scoring_data_list[[scoring_method]],
      family = family,
      prior = prior_settings[[prior_setting]],
      seed = 1234,
      control = list(adapt_delta = 0.95)
    )

    model_results[[scoring_method]][[prior_setting]] <- model_1pl
    ranef_results[[scoring_method]][[prior_setting]] <- ranef(model_1pl)
  }
}

extract_and_combine <- function(ranef_list) {
  do.call(rbind, lapply(names(ranef_list), function(prior_setting) {
    do.call(rbind, lapply(names(ranef_list[[prior_setting]]), function(group) {
      df <- as.data.frame(ranef_list[[prior_setting]][[group]])
      df$prior_setting <- prior_setting
      df$group <- group
      df$item <- rownames(df)
      df
    }))
  }))
}

combined_results <- lapply(ranef_results, extract_and_combine)

combined_data <- do.call(rbind, lapply(names(combined_results), function(scoring_method) {
  df <- combined_results[[scoring_method]]
  df$scoring_method <- scoring_method
  df
}))
```

## Visualize

```{r}
ggplot(combined_data, aes(x = prior_setting, y = Estimate.Intercept, group = interaction(item, scoring_method), color = scoring_method)) +
  geom_line(alpha = 0.5) +
  geom_point(size = 1) +
  labs(title = "Item Difficulties under Different Priors and Scoring Methods",
       x = "Prior Setting",
       y = "Estimate (Intercept)") +
  theme(legend.title = element_blank(),
        legend.position = "bottom")

ggplot(combined_data, aes(x = prior_setting, y = Estimate.Intercept, group = item, color = item)) +
  geom_line(alpha = 0.7) +
  geom_point(size = 1) +
  labs(title = "Item Difficulties under Different Priors and Scoring Methods",
       x = "Prior Setting",
       y = "Estimate (Intercept)") +
  theme(legend.position = "none") +
  facet_wrap(~ scoring_method, ncol = 1)

ggplot(combined_data, aes(x = item, y = Estimate.Intercept, color = scoring_method)) +
  geom_point(size = 2) +
  labs(title = "Item Difficulties under Different Priors and Scoring Methods",
       x = "Item",
       y = "Estimate (Intercept)") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        legend.title = element_blank(),
        legend.position = "bottom") +
  facet_wrap(~ prior_setting, ncol = 2)

ggplot(combined_data, aes(x = item, y = scoring_method, fill = Estimate.Intercept)) +
  geom_tile() +
  labs(title = "Item Difficulties under Different Priors and Scoring Methods",
       x = "Item",
       y = "Scoring Method",
       fill = "Estimate (Intercept)") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1),
        legend.position = "bottom") +
  facet_wrap(~ prior_setting, ncol = 2)

ggplot(combined_data, aes(x = prior_setting, y = Estimate.Intercept, fill = prior_setting)) +
  geom_violin(alpha = 0.7) +
  geom_boxplot(width = 0.1, fill = "white", outlier.shape = NA) +
  labs(title = "Comparison of Item Difficulty Estimates under Different Priors",
       x = "Prior Setting",
       y = "Estimate (Intercept)") +
  theme(legend.position = "none") +
  facet_wrap(~ scoring_method, ncol = 1)

ggplot(combined_data, aes(x = Estimate.Intercept, y = prior_setting, fill = prior_setting)) +
  geom_density_ridges(alpha = 0.7) +
  labs(title = "Comparison of Item Difficulty Estimates under Different Priors",
       x = "Estimate (Intercept)",
       y = "Prior Setting") +
  theme(legend.position = "none") +
  facet_wrap(~ scoring_method, ncol = 1)
```

# Multidimensional IRT

assume that hit and correct rejection are completely different

## Model Specification

```{r}
fit_scoring_2_mirt <- brm(
    Score ~ 1 + (1 | Item) + (0 + Correct_Rejection | Participant),
    data = scoring_2_data,
    family = bernoulli(link = "logit"),
    prior = prior,
    seed = 1234)
```

## Model Results
```{r}
summary(fit_scoring_2_mirt)
```

## Model Comparison
perform model comparison via approximate LOO-CV

```{r}
loo_cv_scoring_2_mirt <- brms::loo(fit_scoring_2_mirt)
```

```{r}
print(loo_cv_scoring_2_mirt, simplify = FALSE)
```


## Visualization
### trace plot

```{r}
plot(fit_scoring_2_mirt, combo = c("trace", "dens_overlay"), ask = F)
```


### posterior predictive checks

```{r}
pp_check(
  object = fit_scoring_2_mirt,
  ndraws = 1000,
  type = "stat",
  stat = "mean") +
  theme_test() +
  ylab("Frequency")

pp_check(
  object = fit_scoring_2_mirt,
  ndraws = 100) +
  theme_test() +
  ylab("Density")
```


## Item Difficulty and Person Ability

extract item and person parameters

```{r}
ranef_scoring_2_mirt <- ranef(fit_scoring_2_mirt)
```

item parameter

```{r}
item_pars_scoring_2_mirt <- ranef_scoring_2_mirt$Item
```

person parameter

```{r}
person_pars_scoring_2_mirt <- ranef_scoring_2_mirt$Participant
```

```{r}
coef_scoring_2_mirt <- coef(fit_scoring_2_mirt)$Participant
```

## Visualization

```{r}
ranef_scoring_2_mirt$Participant[, , "Correct_Rejection"] %>%
  as_tibble() %>%
  rownames_to_column() %>%
  arrange(Estimate) %>%
  mutate(rowname = seq_len(n())) %>%
  ggplot(aes(rowname, Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_pointrange(alpha = 0.7) +
  coord_flip() +
  labs(x = "Person Number (Sorted)")

ranef_scoring_2_mirt$Item[, , "Intercept"] %>%
  as_tibble() %>%
  rownames_to_column() %>%
  arrange(Estimate) %>%
  mutate(rowname = seq_len(n())) %>%
  ggplot(aes(rowname, Estimate, ymin = Q2.5, ymax = Q97.5)) +
  geom_pointrange(alpha = 0.7) +
  coord_flip() +
  labs(x = "Item Number (Sorted)")
```

### Test Information Curve

Define ability range

```{r}
theta_range <- seq(-6, 6, length.out = 100)
```

Define item information function

```{r}
item_info <- function(b, theta) {
  p_theta <- 1 / (1 + exp(-(theta - b)))
  return(p_theta * (1 - p_theta))
}
```

Calculate item information for each item

```{r}
calc_item_info <- function(b_participant, theta_range) {
  info_list <- lapply(seq_along(theta_range), function(t) {
    theta = theta_range[t]
    mean_info = sum(sapply(b_participant[, "Estimate"], item_info, theta = theta))
    lower_info = sum(sapply(b_participant[, "Q2.5"], item_info, theta = theta))
    upper_info = sum(sapply(b_participant[, "Q97.5"], item_info, theta = theta))
    return(c(mean = mean_info, lower = lower_info, upper = upper_info))
  })
  return(do.call(rbind, info_list))
}

# Extract participant ability parameters for each scoring method
b_participant_scoring_2_mirt <- coef(fit_scoring_2_mirt)$Participant[, ,"Correct_Rejection"]

# Calculate item information for each scoring method
info_results_scoring_2_mirt <- calc_item_info(b_participant_scoring_2_mirt, theta_range)

# Convert the results to data frames for plotting
info_df_scoring_2_mirt <- as.data.frame(info_results_scoring_2_mirt)
info_df_scoring_2_mirt$theta <- theta_range

info_df_scoring_2_mirt <- info_df_scoring_2_mirt %>%
  mutate(lower = pmin(lower, mean),
         upper = pmax(upper, mean))
```

maximum test information and theta

```{r}
max_scoring_2_mirt <- max(info_df_scoring_2_mirt$upper)
max_scoring_2_mirt_row <- info_df_scoring_2_mirt[info_df_scoring_2_mirt$upper == max_scoring_2_mirt, ]
max_theta_scoring_2_mirt <- max_scoring_2_mirt_row$theta
max_mean_scoring_2_mirt <- max_scoring_2_mirt_row$mean
```

```{r}
cat("Theta for maximum test information in Scoring 2 with MIRT:", max_theta_scoring_2_mirt, "\nMean for maximum test information in Scoring 2 with MIRT:", max_mean_scoring_2_mirt)
```

## Visualization

```{r}
ggplot(info_df_scoring_2_mirt, aes(x = theta, y = mean)) +
  geom_line(color = "green") +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2, fill = "green") +
  labs(title = "Item Information Function (Scoring 2 with MIRT)", x = "Theta", y = "Information")

# Add a scoring method identifier to each data frame
info_df_scoring_2_mirt <- info_df_scoring_2_mirt %>% mutate(Scoring_Method = "Scoring_2_mirt")

# Combine all data frames into one
combined_info_df_mirt <- bind_rows(info_df_scoring_2, info_df_scoring_2_mirt)

ggplot(combined_info_df_mirt, aes(x = theta, y = mean, color = Scoring_Method, fill = Scoring_Method)) +
  geom_line() +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
  labs(title = "Item Information Function by Scoring Method", x = "Theta", y = "Information") +
  scale_color_manual(values = c("Scoring_2" = "red", "Scoring_2_mirt" = "green")) +
  scale_fill_manual(values = c("Scoring_2" = "red", "Scoring_2_mirt" = "green"))
```
